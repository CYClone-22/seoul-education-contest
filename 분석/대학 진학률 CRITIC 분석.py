# ----------------------------------------
# CRITIC ê°€ì¤‘ì¹˜ + PCA ê¸°ë°˜ ì§„í•™ë¥  ë¶„ì„
# Google Colab í™˜ê²½
# ----------------------------------------

# ğŸ“¦ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (Colab ì „ìš©)
!pip install pandas numpy scikit-learn matplotlib seaborn

# ğŸ“š ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns

# ----------------------------------------
# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
# ----------------------------------------
file_path = '/content/drive/MyDrive/ì„œìš¸êµìœ¡ ë°ì´í„° ë¶„ì„/1ì°¨ ì •ë¦¬ë³¸/ì •ê·œí™”/ê³ ë“±í•™êµë³„_ì§„í•™ë¥ _ì •ê·œí™”.csv'
df = pd.read_csv(file_path)

print("âœ… ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
print(df.head())
print(df.columns)

# ----------------------------------------
# CRITIC ê°€ì¤‘ì¹˜ ê³„ì‚° í•¨ìˆ˜
# ----------------------------------------
def calculate_critic_weights(data):
    """CRITIC ê°€ì¤‘ì¹˜ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜"""
    std_dev = np.std(data, axis=0)
    corr_matrix = np.corrcoef(data, rowvar=False)
    corr_sum = np.sum(1 - np.abs(corr_matrix), axis=0)   # ìƒê´€ì„±ì´ ë‚®ì„ìˆ˜ë¡ ê°€ì¤‘ì¹˜â†‘
    critic_scores = std_dev * corr_sum
    total = np.sum(critic_scores)

    if total == 0 or np.isnan(total):
        return np.ones_like(critic_scores) / len(critic_scores)
    else:
        return critic_scores / total

# ----------------------------------------
# ë¶„ì„ ëŒ€ìƒ ì„¤ì •
# ----------------------------------------
year_col = 'ì—°ë„'
region_col = 'í•™êµ°'
school_col = 'ê³ ë“±í•™êµë³„(2)'
features = ['ì§„í•™ë¥ ']

results = []

# ----------------------------------------
# í•™êµ°ë³„ ë¶„ì„ ì§„í–‰
# ----------------------------------------
for hakgun, group in df.groupby(region_col):
    X = group[features].values

    # ë°ì´í„° í‘œì¤€í™”
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # CRITIC ê°€ì¤‘ì¹˜ ê³„ì‚°
    critic_weights = calculate_critic_weights(X_scaled)

    # PCA ì ìš©
    pca = PCA()
    pca_scores = pca.fit_transform(X_scaled)

    # CRITIC ê°€ì¤‘ì¹˜ ì ìš©
    weighted_scores = pca_scores * critic_weights

    # ê²°ê³¼ ì €ì¥
    group_result = pd.DataFrame(
        weighted_scores,
        columns=[f'{hakgun}_PC{i+1}' for i in range(pca_scores.shape[1])]
    )
    group_result[region_col] = hakgun
    group_result[year_col] = group[year_col].values
    group_result[school_col] = group[school_col].values

    results.append(group_result)

# ----------------------------------------
# ìµœì¢… ê²°ê³¼ ë³‘í•© + ì €ì¥
# ----------------------------------------
final_df = pd.concat(results, ignore_index=True)

save_path = '/content/drive/MyDrive/ì„œìš¸êµìœ¡ ë°ì´í„° ë¶„ì„/1ì°¨ ì •ë¦¬ë³¸/ì˜ì—­3_ì§„í•™ë¥ .csv'
final_df.to_csv(save_path, index=False, encoding='utf-8-sig')

print(final_df.head())
